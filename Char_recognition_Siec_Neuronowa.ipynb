{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12707879224419279038\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4922553139\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16198730809252102227\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 128\n",
    "batch_size = 32\n",
    "data_dir = 'blockCharactersData/Fnt_vol2'\n",
    "model = None\n",
    "train = None\n",
    "val = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35858 files belonging to 36 classes.\n",
      "Using 28687 files for training.\n",
      "Found 35858 files belonging to 36 classes.\n",
      "Using 7171 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=0,\n",
    "    image_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb'\n",
    ")\n",
    "val = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=0,\n",
    "    image_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SieÄ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1/255, input_shape=(img_size, img_size, 3)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size=(5, 5), activation='relu', padding=\"valid\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(64, kernel_size=(5, 5), activation='relu', padding=\"valid\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(36, activation='softmax'))\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling_14 (Rescaling)     (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 128, 128, 16)      448       \n",
      "_________________________________________________________________\n",
      "batch_normalization_82 (Batc (None, 128, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 60, 60, 32)        12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_83 (Batc (None, 60, 60, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 26, 26, 64)        51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 512)               5538304   \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 36)                4644      \n",
      "=================================================================\n",
      "Total params: 5,775,748\n",
      "Trainable params: 5,773,732\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie oraz ocena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "897/897 [==============================] - 56s 61ms/step - loss: 1.0361 - accuracy: 0.7420 - val_loss: 0.2458 - val_accuracy: 0.9311\n",
      "Epoch 2/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.2347 - accuracy: 0.9313 - val_loss: 0.1380 - val_accuracy: 0.9536\n",
      "Epoch 3/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.1609 - accuracy: 0.9505 - val_loss: 0.1076 - val_accuracy: 0.9632\n",
      "Epoch 4/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.1125 - accuracy: 0.9625 - val_loss: 0.0891 - val_accuracy: 0.9667\n",
      "Epoch 5/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.1040 - accuracy: 0.9652 - val_loss: 0.0804 - val_accuracy: 0.9707\n",
      "Epoch 6/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.0893 - accuracy: 0.9685 - val_loss: 0.0606 - val_accuracy: 0.9794\n",
      "Epoch 7/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.0815 - accuracy: 0.9730 - val_loss: 0.1068 - val_accuracy: 0.9720\n",
      "Epoch 8/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.0743 - accuracy: 0.9741 - val_loss: 0.0624 - val_accuracy: 0.9780\n",
      "Epoch 9/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.0655 - accuracy: 0.9776 - val_loss: 0.0643 - val_accuracy: 0.9777\n",
      "Epoch 10/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.0597 - accuracy: 0.9775 - val_loss: 0.0548 - val_accuracy: 0.9794\n",
      "Epoch 11/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.0559 - accuracy: 0.9804 - val_loss: 0.0543 - val_accuracy: 0.9808\n",
      "Epoch 12/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.0545 - accuracy: 0.9806 - val_loss: 0.0563 - val_accuracy: 0.9827\n",
      "Epoch 13/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.0582 - accuracy: 0.9809 - val_loss: 0.0505 - val_accuracy: 0.9828\n",
      "Epoch 14/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.0390 - accuracy: 0.9859 - val_loss: 0.0589 - val_accuracy: 0.9833\n",
      "Epoch 15/15\n",
      "897/897 [==============================] - 54s 60ms/step - loss: 0.0496 - accuracy: 0.9817 - val_loss: 0.0534 - val_accuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c026f55be0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, epochs=15, validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [==============================] - 30s 33ms/step - loss: 0.0228 - accuracy: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02284085378050804, 0.9910063743591309]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wyniki na zbiorze uczÄ…cym\n",
    "model.evaluate(train) #97.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 8s 33ms/step - loss: 0.0534 - accuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05344865471124649, 0.9814530611038208]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wyniki na zbiorze walidacyjnym\n",
    "model.evaluate(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wczytywanie i zapisywanie modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('siec_v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('siec_v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testowanie jednostkowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'blockCharactersData/Fnt_vol2/A/1.png'\n",
    "#path = 'in/17.jpg'\n",
    "path = 'CharExample/44.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'a', 'B', 'b', 'C', 'c', 'D', 'd', 'E', 'e', 'F', 'f', 'G', 'g', 'H', 'h', 'I', 'i', 'J', 'j', 'K', 'k', 'L', 'l', 'M', 'm', 'N', 'n', 'O', 'o', 'P', 'p', 'Q', 'q', 'R', 'r', 'S', 's', 'T', 't', 'U', 'u', 'V', 'v', 'W', 'w', 'X', 'x', 'Y', 'y', 'Z', 'z']\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "labels.extend(list('0123456789'))\n",
    "labels.extend(list('AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz'))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 a\n"
     ]
    }
   ],
   "source": [
    "image = tf.keras.preprocessing.image.load_img(path, target_size=(img_size, img_size))\n",
    "input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "input_arr_batch = np.array([input_arr])\n",
    "predictions = model.predict(input_arr_batch)\n",
    "print(np.argmax(predictions),labels[np.argmax(predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread(path)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}